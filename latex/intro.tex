\label{sec:intro}

Support Vector Machine (SVM)~\cite{cortes1995support} is a non-parametric classification algorithm widely used when solving pattern recognition tasks. The goal of this report is to explain SVM as a convex optimization problem and analyze the complexity of the model as a function of a regularization parameter.
\subsection{Support Vector Machine with hard constraints}
We are given a training dataset $\{x_i, t_i\}$, $i=1\ldots m$, where $x_i \in \mathbb{R}^n$ is the point coordinates and  $t_i \in \{-1,+1 \}$ is the associated label. For now, we assume that the data is linearly separable, i.e., there exists at least one hyperplane that can separates the two classes. The goal is to find the hyperplane that maximizes the margin, which is the smallest distance from each class to the hyperplane.

The hyperplane is defined by the equation: $w^Tx+b$, such that:
%\[
%  \begin{cases} 
%   w^Tx_i+b \leq \gamma & \text{if } t_i=+1 \\
%   w^Tx_i+b \geq -\gamma       & \text{if } t_i=-1
%  \end{cases}
%\]
%Since we can always normalize $w$ and $b$ by $\gamma$, the  previous conditions can be written as:
\[
  \begin{cases} 
   w^Tx_i+b \geq 1 & \text{if } t_i=+1 \\
   w^Tx_i+b \leq -1       & \text{if } t_i=-1
  \end{cases}
\]

This can be merged into one condition: $ t_i(w^Tx_i+b) \geq 1$.

We want to maximize the margin, which is equal to $\frac{1}{\|w\|}$. This is equivalent to minimizing $\|w\|^2$, which is a more convenient form as it is a quadratic objective function. The optimization problem is the following:
% $w^Tx_i+b \leq \gamma$ if $t_i=+1$ and  $w^Tx_i+b \geq -\gamma$ if $t_i=-1$. Since we can always normalize $w$ and $b$ by $\gamma$, the  previous conditions can be written as $w^Tx_i+b \leq 1$ if $t_i=+1$ and  $w^Tx_i+b \geq -1$ if $t_i=-1$. In this case, the margin is $\frac{1}{\|w\|}$. 
\begin{equation}
\begin{aligned}
& \underset{w}{\text{minimize}}
& & \frac{1}{2}\|w\|^2 \\
& \text{subject to}
& & t_i(w^Tx_i+b) \geq 1, \; i = 1, \ldots, m.
\label{eq:svm_hard}
\end{aligned}
\end{equation}

This is a quadratic programming problem, which can be written with cvx as:
\begin{verbatim}
cvx_begin
    variables w(dim) b; 
    dual variable alpha;
    minimize(0.5*w'*w);
    subject to 
  	    y: t.*(x*w+b) > 1; 
cvx_end
\end{verbatim}
The Lagrangian is defined as the following: 
\begin{equation}
  \mathcal{L}(w,b,\alpha)= \frac{1}{2} \|w\|^2+\sum\limits_{i=1}^m \alpha_i (1-t_i(w^Tx_i+b))
\end{equation}

According to the KKT dual complementarity slackeness condition, $\alpha_i = 0$ when $t_i(w^Tx_i+b) > 1$. This implies that  $\alpha_i \neq 0$ only for the points lying on the margin. These points are called Support Vectors, as they are the only ones that will have an impact on the optimal solution.\\
To derive the dual problem we set:
%\[g(\alpha)=\underset{w,b}{\inf}\mathcal{L}(w,b,\alpha)\].
\begin{equation}
\begin{aligned}
 \nabla _w \mathcal{L}(w,b,\alpha)&=0 \Leftrightarrow w=\sum\limits_{i=1}^m \alpha_i t_i x_i\\
\frac{ \partial \mathcal{L}(w,b,\alpha)}{\partial b}&=0 \Leftrightarrow  \sum\limits_{i=1}^m \alpha_i t_i=0
\end{aligned}
\end{equation}

Thus, the dual problem is:
\begin{equation}
\begin{aligned}
& \underset{w}{\text{maximize}}
& & \sum\limits_{i=1}^m \alpha_i - \frac{1}{2} \sum\limits_{i=1}^m \sum\limits_{j=1}^m \alpha_i \alpha_j t_i t_j x_i^T x_j \\
& \text{subject to}
& & \alpha_i \geq 0, \; i = 1, \ldots, m \\
& & & \sum\limits_{i=1}^m \alpha_i t_i = 0.
\end{aligned}
\end{equation}

\subsection{Constraints relaxation}
There are two obvious issues with this approach. The first one is that the optimization problem is infeasible when the data is not linearly separable, the second one is that the optimal solution is very sensitive to outliers. To palliate to these issues, we need to relax the margin constraint. To do so, slack variables $\xi_i$, $i=1,...,m$ are introduced and the optimization problem becomes:
\begin{equation}
\begin{aligned}
& \underset{w}{\text{minimize}}
& & \frac{1}{2}\|w\|^2 + C\sum\limits_{i=1}^m \xi_i\\
& \text{subject to}
& & t_i(w^Tx_i+b) \geq 1-\xi_i, \; i = 1, \ldots, m\\
& & & \xi_i \geq 0, \; i = 1, \ldots, m.
\label{eq:svm_soft}
\end{aligned}
\end{equation}

In this scenario, points are permitted to have a margin that is less than 1. If a point has a margin of $1 - \xi_i$, then the cost function will be increased by $C\xi_i$. 
\\The corresponding cvx code is:
\begin{verbatim}
cvx_begin
    variables w(dim) b slack(n);
    dual variable alpha;
    minimize(0.5*w'*w + C*sum(slack));
    subject to 
  	    y: t.*(x*w+b) > 1 - slack; 
        slack > 0;
  cvx_end
\end{verbatim}
With this relaxed margin constraint, the dual problem becomes:
\begin{equation}
\begin{aligned}
& \underset{w}{\text{maximize}}
& & \sum\limits_{i=1}^m \alpha_i - \frac{1}{2} \sum\limits_{i=1}^m \sum\limits_{j=1}^m \alpha_i \alpha_j t_i t_j x_i^T x_j \\
& \text{subject to}
& & 0 \leq \alpha_i \leq C, \; i = 1, \ldots, m \\
& & & \sum\limits_{i=1}^m \alpha_i t_i = 0.
\end{aligned}
\end{equation}

\subsection{Multiclass}
Until now, we assumed that the data belonged to two classes with labels $t_i=\pm1$. To be able to use this method with $K$ classes, one way is to transform the multiclass problem into  $K$ binary classification problems by using the one-versus-all strategy. 

In each binary classification problem, we assume that the points belonging to a given class $k$, with $k=1\ldots K$, is labelled as $t_i=1$ and all the other points will be labelled as $t_i=-1$. Once all the binary classifiers have been trained, we need to decide which class to choose for a given point $x_i$, $i=1\ldots m$. To do so, we pick the class $k$ such that:
\[k = \arg\max_{1 \leq k \leq K} w_k^Tx_i+b_k.\] 