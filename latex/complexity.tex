\label{sec:complexity}

As discussed in Section \ref{sec:intro}, the goal of the constraints relaxation in SVMs is to maximize the margins while softly penalizing data points that lie in the wrong side of the margins borders.
In Equation \ref{eq:svm_soft}, this penalty adds to the cost function $C\xi{i}$.
The value of $C$ can be interpreted as a regularization parameter.
Low values for $C$ ``allows'' more data points lying between the margins, making them ``softer''.
On the other hand, higher values of $C$ makes the margins ``harder''.
Some examples of how the margins look like for different values of $C$ can be seen in Figure \ref{fig:svm}.
In this example made with toy data, it is possible to observe the margins getting harder and harder (with less data points between the margins) when $C$ is increased by powers of 10.

\begin{figure}[!htb]
\begin{center}
\includegraphics [width=12.5cm] {./graphics/SVM_PLOT.png}
\caption{Linear SVM is used to find a separation between the red dots and the blue dots under diferent values of $C$. The solid lines are the optimal decision boundaries. The dashed lines represent the margins and the stars are the support vectors ($\alpha_{i} > 0$).} \label{fig:svm}
\end{center}
\end{figure}

%The number of Lagrange variables greater than $0$ ($>0$ not $\geq 0$) will gives us the number of support vectors ().

Since the strong duality holds for this QP problem, we can do some sensitive interpretations with the Lagrange variables $\alpha_{i}$ \cite{boyd2004convex}.
As aforementioned are considered support vectors all dual variables that are greater than 0 ($\alpha_{i} > 0$).
For practical purposes, in our Matlab scripts, we are considering as support vectors all dual variables greater than $0.0000001$ ($ \alpha_{i} > 0.0000001$).
Let's define $r$ as the ratio between the number of support vectors and the number of data points used to train the SVM model.
This value can gives us an intuition about the complexity of the model.
When $r$ is equal to 1, it means that the model needs the whole training data to define a decision boundary.
This makes our model more complex in the sense that it needs as much active constraints as data.

The task of picking a value for $C$ is to select the simplest model that gives the highest accuracy.
Figure \ref{fig:toy} shows a plot with the accuracies (solid line) and the value of $r$ (dashed line) for each value of $C$ in our toy example.

\begin{figure}[!htb]
\begin{center}
\includegraphics [width=10.5cm] {./graphics/toy_plot.png}
\caption{Accuracy and Support Vector Ratio $r$ vs $C$} \label{fig:toy}
\end{center}
\end{figure}

It is possible to observe that the accuracies are relatively stable along the range of $C$, but the complexity of the model starts to decrease at $C=10^{-2}$.
A good trade-off between model complexity and accuracy can be reached with $C$ between $10^1$ and $10^3$, which corresponds to $\approx 20\%$ of the training set as support vectors. 
The plots from Figures \ref{fig:svm} and \ref{fig:toy} can be reproduced executing the script  `toy\_analysis.m'.

The same sensitivity analysis can be done with real data.
Figure \ref{fig:datasets} shows the same, accuracy and $r$, plots for different values of $C$ in three different datasets.
Each dataset is split in two sets.
One set is used to train the linear SVM model and the second set is used just to report the accuracies.
The accuracies in the Figure \ref{fig:toy} are represented by the blue and red solid lines for the train and test sets respectively.

Containing 150 data points split in 3 classes the Iris Dataset\footnote{https://archive.ics.uci.edu/ml/datasets/iris} is a well known database used in the pattern recognition literature.
In Figure \ref{fig:datasets} (a), we observe that a good trade-off is obtained with $C=10^2$ with an accuracy above 90\% in the training set and in the test set.
The plot can be reproduced executing the script `iris\_analysis.m' .

Figure \ref{fig:datasets} (b) presents the same analysis with the Wine Dataset\footnote{https://archive.ics.uci.edu/ml/datasets/wine} (178 data points split in 3 classes).
A good trade-off can be observed with $C$ greater than $1$ with an accuracy above 90\% for both training and test sets.
The plot can be reproduced executing the script `wine\_analysis.m'.

Finally Figure \ref{fig:datasets} (c) presents the analysis with the Column Vertebral Dataset \footnote{https://archive.ics.uci.edu/ml/datasets/Vertebral+Column} (310 data points split in three classes).
The good trade-off can be observed with $C$ equals to $10$ with an accuracy above 80\% for both training and test sets.
The plot can be reproduced executing the script `vertebral\_column\_analysis.m' .

\begin{figure}[!htb]
\begin{center}
\includegraphics [width=8.5cm] {./graphics/iris.png}
\includegraphics [width=8.5cm] {./graphics/wine.png}

\includegraphics [width=8.5cm] {./graphics/vertebral_column.png}
\caption{Accuracy and Support Vector Ratio $r$ vs $C$ for the Iris (a), Wine (b) and Column Vertebral (c) Datasets} \label{fig:datasets}
\end{center}
\end{figure}